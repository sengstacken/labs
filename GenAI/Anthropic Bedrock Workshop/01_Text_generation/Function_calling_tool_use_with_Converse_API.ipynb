{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function-Calling (Tool Use) with Converse API in Amazon Bedrock\n",
    "\n",
    "In this example notebook, we'll explore how to perform Function-Calling through the use of Tools with the Converse API for Amazon Bedrock.\n",
    "\n",
    "The Converse or ConverseStream API is a unified structured text action for simplifying the invocations to Bedrock LLMs. It includes the possibility to define tools for implementing external functions that can be called or triggered from the LLMs.\n",
    "\n",
    "Let's start by installing a few requirements for running this example. We only need to run this cell the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -qU boto3\n",
    "!pip3 install -qU googlesearch-python lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json, sys\n",
    "from datetime import datetime\n",
    "\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a few variables. Make sure to adjust these parameters according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "#modelId = 'cohere.command-r-plus-v1:0'\n",
    "#modelId = 'cohere.command-r-v1:0'\n",
    "#modelId = 'mistral.mistral-large-2402-v1:0'\n",
    "print(f'Using modelId: {modelId}')\n",
    "\n",
    "region = 'us-west-2'\n",
    "print('Using region: ', region)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic tool function-calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to define our tools through Python functions.\n",
    "\n",
    "In our example, we will define a tool for simulating a weather forecast lookup tool (get_weather). Note in our example we're just returning a constant weather forecast to illustrate the concept, but you could make it fully functional by connecting any weather service API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToolsList:\n",
    "    #Define our get_weather tool function...\n",
    "    def get_weather(self, city, state):\n",
    "        #print(city, state)\n",
    "        result = f'Weather in {city}, {state} is 70F and clear skies.'\n",
    "        print(f'Tool result: {result}')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's structure our tools configuration for passing this information to our Converse API later. We have to clearly define the schema that our tools are expecting in the corresponding functions.\n",
    "\n",
    "Note that we can also define specific configurations such as the tool choice, which allow us to either let the LLM choose automatically (auto), or overriding a fixed tool to be called always. You can check more information on this parameter in the Bedrock Converse API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the configuration for our tool...\n",
    "toolConfig = {'tools': [],\n",
    "'toolChoice': {\n",
    "    'auto': {},\n",
    "    #'any': {},\n",
    "    #'tool': {\n",
    "    #    'name': 'get_weather'\n",
    "    #}\n",
    "    }\n",
    "}\n",
    "\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Get weather of a location.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'city': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'City of the location'\n",
    "                        },\n",
    "                        'state': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'State of the location'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['city', 'state']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a handy function for calling Bedrock with the Converse API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for caling the Bedrock Converse API...\n",
    "def converse_with_tools(messages, system='', toolConfig=toolConfig):\n",
    "    response = bedrock.converse(\n",
    "        modelId=modelId,\n",
    "        system=system,\n",
    "        messages=messages,\n",
    "        toolConfig=toolConfig\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready for setting up our orchestration flow. In this case, we'll make a first call to the LLM with the initial prompt from the user, and depending on the answer from the LLM we'll either call a tool (function calling) or end the interaction.\n",
    "\n",
    "Note that in the case the LLM indicates that it wants to run a tool (function calling), it will give us the information required of tool name and arguments for us to run the relevant tool in our code; i.e. The LLMs cannot run the tools automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for orchestrating the conversation flow...\n",
    "def converse(prompt, system=''):\n",
    "    #Add the initial prompt:\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Initial prompt:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "    #Invoke the model the first time:\n",
    "    output = converse_with_tools(messages, system)\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Output so far:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    #Add the intermediate output to the prompt:\n",
    "    messages.append(output['output']['message'])\n",
    "\n",
    "    function_calling = next((c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c), None)\n",
    "\n",
    "    #Check if function calling is triggered:\n",
    "    if function_calling:\n",
    "        #Get the tool name and arguments:\n",
    "        tool_name = function_calling['name']\n",
    "        tool_args = function_calling['input'] or {}\n",
    "        \n",
    "        #Run the tool:\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Running ({tool_name}) tool...\")\n",
    "        tool_response = getattr(ToolsList(), tool_name)(**tool_args) or \"\"\n",
    "        if tool_response:\n",
    "            tool_status = 'success'\n",
    "        else:\n",
    "            tool_status = 'error'\n",
    "\n",
    "        #Add the tool result to the prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId':function_calling['toolUseId'],\n",
    "                            'content': [\n",
    "                                {\n",
    "                                    \"text\": tool_response\n",
    "                                }\n",
    "                            ],\n",
    "                            'status': tool_status\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        #print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Messages so far:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "        #Invoke the model one more time:\n",
    "        output = converse_with_tools(messages, system)\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Final output:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything setup and are ready for testing our function-calling bot.\n",
    "\n",
    "Let's try with a few sample prompts, each one with different needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the weather like in Queens, NY?\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    converse(\n",
    "        system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather'; \\\n",
    "            only use the tool if required. Don't make reference to the tools in your final answer.\"}],\n",
    "        prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the LLM decides whether or not to call the get_weather tool depending on the question.\n",
    "\n",
    "You can further improve this example by playing with the system prompts. Also, you might want to switch the LLM and explore the differences.\n",
    "\n",
    "Note that the Bedrock Converse API allow us to seamlessly use the same code and structure for working with multiple model providers. No changes are needed in the syntax of our interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic tool function-calling, with image result\n",
    "\n",
    "Let's now explore a case where our get_weather tool is not only returning a text, but now also returns an image.\n",
    "\n",
    "For this, we will simulate that our tool is returning a complex visual forecast like the sample image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./images/weather.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToolsList2:\n",
    "    #Define our get_weather tool function...\n",
    "    def get_weather(self, city, state):\n",
    "        #print(city, state)\n",
    "        result = f'Weather in {city}, {state} is 70F and clear skies.'\n",
    "        with open('./images/weather.jpg', \"rb\") as image_file:\n",
    "            binary_data = image_file.read()\n",
    "        return result, binary_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the configuration for our tool...\n",
    "toolConfig = {'tools': [],\n",
    "'toolChoice': {\n",
    "    'auto': {},\n",
    "    #'any': {},\n",
    "    #'tool': {\n",
    "    #    'name': 'get_weather'\n",
    "    #}\n",
    "    }\n",
    "}\n",
    "\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Get weather of a location.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'city': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'City of the location'\n",
    "                        },\n",
    "                        'state': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'State of the location'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['city', 'state']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for orchestrating the conversation flow...\n",
    "def converse_image(prompt, system=''):\n",
    "    #Add the initial prompt:\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Initial prompt:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "    #Invoke the model the first time:\n",
    "    output = converse_with_tools(messages, system)\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Output so far:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    #Add the intermediate output to the prompt:\n",
    "    messages.append(output['output']['message'])\n",
    "\n",
    "    function_calling = next((c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c), None)\n",
    "\n",
    "    #Check if function calling is triggered:\n",
    "    if function_calling:\n",
    "        #Get the tool name and arguments:\n",
    "        tool_name = function_calling['name']\n",
    "        tool_args = function_calling['input'] or {}\n",
    "        \n",
    "        #Run the tool:\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Running ({tool_name}) tool...\")\n",
    "        tool_response, tool_response_image = getattr(ToolsList2(), tool_name)(**tool_args)\n",
    "\n",
    "        #Add the tool result to the prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId':function_calling['toolUseId'],\n",
    "                            'content': [\n",
    "                                {\n",
    "                                    \"text\": tool_response\n",
    "                                },\n",
    "                                {\n",
    "                                    \"image\": {\n",
    "                                        \"format\": \"jpeg\",\n",
    "                                        \"source\":  {\n",
    "                                            \"bytes\": tool_response_image,\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Invoke the model one more time:\n",
    "        output = converse_with_tools(messages, system)\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Final output:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the weather like in Seattle?\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    converse_image(\n",
    "        system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations called 'get_weather'; \\\n",
    "            only use this tool if required. Don't make reference to the tools in your final answer.\"}],\n",
    "        prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is now considering both the text and image returned by our get_weather tool for providing its final answer.\n",
    "\n",
    "You can play with the system and user prompts for experimenting other scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-tool choice function-calling\n",
    "\n",
    "Finally, let's explore the case where we not only have a get_weather tool, but now also have a fully functional web_search tool that can look up information in the Internet for enriching our responses.\n",
    "\n",
    "For this, we'll leverage a public library with a simple web scrapping implementation. You might to adjust this example for using an API-key based library for improved efficiency in the web search, but this is out of the scope of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ToolsList3:\n",
    "    #Define our get_weather tool function...\n",
    "    def get_weather(self, city, state):\n",
    "        #print(city, state)\n",
    "        result = f'Weather in {city}, {state} is 70F and clear skies.'\n",
    "        return result\n",
    "\n",
    "    #Define our web_search tool function...\n",
    "    def web_search(self, query):\n",
    "        #print(f'{datetime.now().strftime(\"%H:%M:%S\")} - Searching for {search_term} on Internet.')\n",
    "        results = []\n",
    "        response_list = []\n",
    "        results.extend([r for r in search(query, 3, 'en')])\n",
    "        for j in results:\n",
    "            response = requests.get(j)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                response_list.append(soup.get_text().strip())\n",
    "        response_text = \",\".join(str(i) for i in response_list)\n",
    "        #print(f'{datetime.now().strftime(\"%H:%M:%S\")} - Search results: {response_text}')\n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the configuration for our tool...\n",
    "toolConfig = {'tools': [],\n",
    "'toolChoice': {\n",
    "    'auto': {},\n",
    "    #'any': {},\n",
    "    #'tool': {\n",
    "    #    'name': 'get_weather'\n",
    "    #}\n",
    "    }\n",
    "}\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Get weather of a location.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'city': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'City of the location'\n",
    "                        },\n",
    "                        'state': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'State of the location'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['city', 'state']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'web_search',\n",
    "            'description': 'Search a term in the public Internet. \\\n",
    "                Useful for getting up to date information.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'search_term': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'Term to search in the Internet'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['search_term']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for orchestrating the conversation flow...\n",
    "def converse_multi(prompt, system=''):\n",
    "    #Add the initial prompt:\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Initial prompt:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "    #Invoke the model the first time:\n",
    "    output = converse_with_tools(messages, system)\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Output so far:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    #Add the intermediate output to the prompt:\n",
    "    messages.append(output['output']['message'])\n",
    "\n",
    "    function_calling = next((c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c), None)\n",
    "\n",
    "    #Check if function calling is triggered:\n",
    "    if function_calling:\n",
    "        #Get the tool name and arguments:\n",
    "        tool_name = function_calling['name']\n",
    "        tool_args = function_calling['input'] or {}\n",
    "        \n",
    "        #Run the tool:\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Running ({tool_name}) tool...\")\n",
    "        tool_response = getattr(ToolsList3(), tool_name)(**tool_args)\n",
    "\n",
    "        #Add the tool result to the prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId':function_calling['toolUseId'],\n",
    "                            'content': [\n",
    "                                {\n",
    "                                    \"text\": tool_response\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Invoke the model one more time:\n",
    "        output = converse_with_tools(messages, system)\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Final output:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the weather like in Queens, NY?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"In which team is Caitlin Clark playing in the WNBA in 2024?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    converse_multi(\n",
    "        system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather', and another tool to perform a web search for up to date information 'web_search'; \\\n",
    "            use those tools if required. Don't mention the tools in your final answer.\"}],\n",
    "        prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the LLM decides whether to call the get_weather tool, provide an answer without any tool, or searching in the public Internet with the web_search tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can further improve this example by adjusting the parameters of the web search, playing with the system & user prompts, or adjusting the 'toolChoice' parameter of the Converse API tool use definition. You can find further information about these in the Bedrock documentation.\n",
    "\n",
    "Note that the Bedrock Converse API allow us to seamlessly use the same code and structure for working with multiple model providers. No changes are needed in the syntax of our interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parallel function calling with Pydantic definition\n",
    "\n",
    "Let's explore the case where we need to run multiple tools, in which is known as parallel function calling (actually the calling is done by us in sequence, but that's the common term for it).\n",
    "\n",
    "Also, for avoiding the effort and potential mistakes of defining our tool config schema in a JSON dictionary, we'll rely on a Pydantic class that will build it for us from a decorated function.\n",
    "\n",
    "Let's start by installing Pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -qU pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import a few libraries that are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "import inspect\n",
    "import boto3\n",
    "from pydantic import BaseModel, Field, create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup our constants and boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = \"us-west-2\"\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define our Pydantic-based helper function for doing the tool config translation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bedrock_tool(name, description):\n",
    "    def decorator(func):\n",
    "        input_model = create_model(\n",
    "            func.__name__ + \"_input\",\n",
    "            **{\n",
    "                name: (param.annotation, param.default)\n",
    "                for name, param in inspect.signature(func).parameters.items()\n",
    "                if param.default is not inspect.Parameter.empty\n",
    "            },\n",
    "        )\n",
    "\n",
    "        func.bedrock_schema = {\n",
    "            'toolSpec': {\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'inputSchema': {\n",
    "                    'json': input_model.schema()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our tool list, and all we need to specify is the name, description, any relevant attributes that are required in our function.\n",
    "\n",
    "Of course we should also add the content of our tool, in our case we'll keep using the mock get_weather example.\n",
    "\n",
    "If you have more tools, you can adapt this cell for testing with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### DEFINE YOUR TOOLS HERE  ###\n",
    "class ToolsList:\n",
    "    @bedrock_tool(\n",
    "        name=\"get_weather\",\n",
    "        description=\"Get weather of a location.\"\n",
    "    )\n",
    "    def get_weather(self, city: str = Field(..., description=\"City of the location\"),\n",
    "                    state: str = Field(..., description=\"State of the location\")):\n",
    "        result = f'Weather in {city, state} is 70F and clear skies.'\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to define our configuration, setup the function for invoking Bedrock with Converse, and define our workflow orchestration function as per the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toolConfig = {\n",
    "    'tools': [tool.bedrock_schema for tool in ToolsList.__dict__.values() if hasattr(tool, 'bedrock_schema')],\n",
    "    'toolChoice': {'auto': {}}\n",
    "}\n",
    "\n",
    "def converse_with_tools(modelId, messages, system='', toolConfig=None):\n",
    "    return bedrock.converse(\n",
    "        modelId=modelId,\n",
    "        system=system,\n",
    "        messages=messages,\n",
    "        toolConfig=toolConfig\n",
    "    )\n",
    "\n",
    "def converse(tool_class, modelId, prompt, system='', toolConfig=None):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    print(f\"{datetime.now():%H:%M:%S} - Invoking model...\")\n",
    "    output = converse_with_tools(modelId, messages, system, toolConfig)\n",
    "    messages.append(output['output']['message'])\n",
    "    print(f\"{datetime.now():%H:%M:%S} - Got output from model...\")\n",
    "\n",
    "    function_calling = [c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c]\n",
    "    if function_calling:\n",
    "        tool_result_message = {\"role\": \"user\", \"content\": []}\n",
    "        for function in function_calling:\n",
    "            print(f\"{datetime.now():%H:%M:%S} - Function calling - Calling tool...\")\n",
    "            tool_name = function['name']\n",
    "            tool_args = function['input'] or {}\n",
    "            tool_response = getattr(tool_class, tool_name)(**tool_args)\n",
    "            print(f\"{datetime.now():%H:%M:%S} - Function calling - Got tool response...\")\n",
    "            tool_result_message['content'].append({\n",
    "                'toolResult': {\n",
    "                    'toolUseId': function['toolUseId'],\n",
    "                    'content': [{\"text\": tool_response}]\n",
    "                }\n",
    "            })\n",
    "        messages.append(tool_result_message)\n",
    "        print(f\"{datetime.now():%H:%M:%S} - Function calling - Calling model with result...\")\n",
    "        \n",
    "        output = converse_with_tools(modelId, messages, system, toolConfig)\n",
    "        messages.append(output['output']['message'])\n",
    "        print(f\"{datetime.now():%H:%M:%S} - Function calling - Got final answer.\")\n",
    "\n",
    "    return messages, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test our example with a sample prompt. Note we're asking about the weather in 2 cities at the same time, and the LLM is capable on knowing that it must call our tool 2 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ADJUST YOUR SYSTEM PROMPT HERE - IF DESIRED ###\n",
    "system_prompt = [{\"text\": \"You're provided with a tool that can get the weather information for a specific location 'get_weather'; \\\n",
    "                              only use the tool if required. You can call the tool multiple times in the same response if required. \\\n",
    "                              Don't make reference to the tools in your final answer.\"}]\n",
    "### REPLACE WITH YOUR OWN PROMPTS HERE ###\n",
    "prompt = \"What is the weather in Paris and in Berlin?\"\n",
    "\n",
    "messages, output = converse(ToolsList(), MODEL_ID, prompt, system_prompt, toolConfig)\n",
    "print(output)\n",
    "print(f\"Output:\\n{output['output']['message']['content'][0]['text']}\\n\")\n",
    "print(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more details about the parameters supported in the Bedrock Converse API, read the documentation [here](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) and [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html)."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
