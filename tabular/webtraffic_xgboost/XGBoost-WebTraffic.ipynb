{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this workshop, we will go through the steps of training, debugging, deploying and monitoring a **network traffic classification model**.\n",
    "\n",
    "For training our model we will be using datasets <a href=\"https://registry.opendata.aws/cse-cic-ids2018/\">CSE-CIC-IDS2018</a> by CIC and ISCX which are used for security testing and malware prevention.\n",
    "These datasets include a huge amount of raw network traffic logs, plus pre-processed data where network connections have been reconstructed and  relevant features have been extracted using CICFlowMeter, a tool that outputs network connection features as CSV files. Each record is classified as benign traffic, or it can be malicious traffic, with a total number of 15 classes.\n",
    "\n",
    "Starting from this featurized dataset, we have executed additional pre-processing for the purpose of this lab:\n",
    "<ul>\n",
    "    <li>Encoded class labels</li>\n",
    "    <li>Replaced invalid string attribute values generated by CICFlowMeter (e.g. inf and Infinity)</li>\n",
    "    <li>Executed one hot encoding of discrete attributes</li>\n",
    "    <li>Remove invalid headers logged multiple times in the same CSV file</li>\n",
    "    <li>Reduced the size of the featurized dataset to ~1.3GB (from ~6.3GB) to speed-up training, while making sure that all classes are well represented</li>\n",
    "    <li>Executed stratified random split of the dataset into training (80%) and validation (20%) sets</li>\n",
    "</ul>\n",
    "\n",
    "Class are represented and have been encoded as follows (train + validation):\n",
    "\n",
    "\n",
    "| Label                    | Encoded | N. records |\n",
    "|:-------------------------|:-------:|-----------:|\n",
    "| Benign                   |    0    |    1000000 |\n",
    "| Bot                      |    1    |     200000 |\n",
    "| DoS attacks-GoldenEye    |    2    |      40000 |\n",
    "| DoS attacks-Slowloris    |    3    |      10000 |\n",
    "| DDoS attacks-LOIC-HTTP   |    4    |     300000 |\n",
    "| Infilteration            |    5    |     150000 |\n",
    "| DDOS attack-LOIC-UDP     |    6    |       1730 |\n",
    "| DDOS attack-HOIC         |    7    |     300000 |\n",
    "| Brute Force -Web         |    8    |        611 |\n",
    "| Brute Force -XSS         |    9    |        230 |\n",
    "| SQL Injection            |   10    |         87 |\n",
    "| DoS attacks-SlowHTTPTest |   11    |     100000 |\n",
    "| DoS attacks-Hulk         |   12    |     250000 |\n",
    "| FTP-BruteForce           |   13    |     150000 |\n",
    "| SSH-Bruteforce           |   14    |     150000 |       \n",
    "\n",
    "The final pre-processed dataset has been saved to a public Amazon S3 bucket for your convenience, and will represent the inputs to the training processes.\n",
    "\n",
    "### Let's get started!\n",
    "\n",
    "First, we set some variables, including the AWS region we are working in, the IAM (Identity and Access Management) execution role of the notebook instance and the Amazon S3 bucket where we will store data, models, outputs, etc. We will use the Amazon SageMaker default bucket for the selected AWS region, and then define a key prefix to make sure all objects have share the same prefix for easier discoverability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n",
      "arn:aws:iam::431615879134:role/sagemaker-test-role\n",
      "sagemaker-us-east-1-431615879134\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "prefix = 'xgboost-webtraffic'\n",
    "os.environ[\"AWS_REGION\"] = region\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can copy the dataset from the public Amazon S3 bucket to the Amazon SageMaker default bucket used in this workshop. To do this, we will leverage on the AWS Python SDK (boto3) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "source_bucket_name = \"endtoendmlapp\"\n",
    "source_bucket_prefix = \"aim362/data/\"\n",
    "source_bucket = s3.Bucket(source_bucket_name)\n",
    "\n",
    "for s3_object in source_bucket.objects.filter(Prefix=source_bucket_prefix):\n",
    "    copy_source = {\n",
    "        'Bucket': source_bucket_name,\n",
    "        'Key': s3_object.key\n",
    "    }\n",
    "    print('Copying {0} ...'.format(s3_object.key))\n",
    "    s3.Bucket(bucket_name).copy(copy_source, prefix+'/data/'+s3_object.key.split('/')[-2]+'/'+s3_object.key.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download some of the data to the notebook to quickly explore the dataset structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 's3://' + bucket_name + '/' + prefix + '/data/train/0.part'\n",
    "val_file_path = 's3://' + bucket_name + '/' + prefix + '/data/val/0.part'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_file_path)\n",
    "print(val_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/train/ data/val/\n",
    "!aws s3 cp {train_file_path} data/train/ \n",
    "!aws s3 cp {val_file_path} data/val/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "df = pd.read_csv('data/train/0.part')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('data/val/0.part')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training\n",
    "\n",
    "We will execute the training using the built in XGBoost algorithm.  Not that you can also use script mode if you need to have greater customization of the training process.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.image_uris.retrieve('xgboost',boto3.Session().region_name,version='1.0-1')\n",
    "\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/data/train'.format(bucket_name, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/data/val'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"3\",\n",
    "    \"eta\": \"0.1\",\n",
    "    \"gamma\": \"6\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": \"15\",\n",
    "    \"num_round\": \"10\"\n",
    "}\n",
    "\n",
    "output_path = 's3://{0}/{1}/output/'.format(bucket_name, prefix)\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-26 19:46:34 Starting - Starting the training job...\n",
      "2021-05-26 19:46:42 Starting - Launching requested ML instancesProfilerReport-1622058394: InProgress\n",
      ".........\n",
      "2021-05-26 19:48:16 Starting - Preparing the instances for training......\n",
      "2021-05-26 19:49:16 Downloading - Downloading input data...\n",
      "2021-05-26 19:49:56 Training - Training image download completed. Training in progress..\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[19:50:06] 2122136x84 matrix with 178259424 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[19:50:09] 530542x84 matrix with 44565528 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 2122136 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 530542 rows\u001b[0m\n",
      "\u001b[34m[19:50:09] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[0]#011train-merror:0.04227#011validation-merror:0.04247\u001b[0m\n",
      "\u001b[34m[1]#011train-merror:0.03837#011validation-merror:0.03857\u001b[0m\n",
      "\u001b[34m[2]#011train-merror:0.03940#011validation-merror:0.03967\u001b[0m\n",
      "\u001b[34m[3]#011train-merror:0.03920#011validation-merror:0.03950\u001b[0m\n",
      "\u001b[34m[4]#011train-merror:0.03487#011validation-merror:0.03526\u001b[0m\n",
      "\u001b[34m[5]#011train-merror:0.03640#011validation-merror:0.03680\u001b[0m\n",
      "\u001b[34m[6]#011train-merror:0.03442#011validation-merror:0.03483\u001b[0m\n",
      "\u001b[34m[7]#011train-merror:0.03440#011validation-merror:0.03485\u001b[0m\n",
      "\u001b[34m[8]#011train-merror:0.03394#011validation-merror:0.03436\u001b[0m\n",
      "\n",
      "2021-05-26 19:53:37 Uploading - Uploading generated training model\n",
      "2021-05-26 19:53:37 Completed - Training job completed\n",
      "\u001b[34m[9]#011train-merror:0.03298#011validation-merror:0.03338\u001b[0m\n",
      "Training seconds: 263\n",
      "Billable seconds: 263\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure that our code works for inference, we can deploy the trained model and execute some inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                            instance_type='ml.m4.xlarge',serializer=sagemaker.serializers.CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We expect 4 - DDoS attacks-LOIC-HTTP as the predicted class for this instance.\n",
    "test_values = [80,1056736,3,4,20,964,20,0,6.666666667,11.54700538,964,0,241.0,482.0,931.1691850999999,6.6241710320000005,176122.6667,431204.4454,1056315,2,394,197.0,275.77164469999997,392,2,1056733,352244.3333,609743.1115,1056315,24,0,0,0,0,72,92,2.8389304419999997,3.78524059,0,964,123.0,339.8873763,115523.4286,0,0,1,1,0,0,0,1,1.0,140.5714286,6.666666667,241.0,0.0,0.0,0.0,0.0,0.0,0.0,3,20,4,964,8192,211,1,20,0.0,0.0,0,0,0.0,0.0,0,0,20,2,2018,1,0,1,0]\n",
    "result = predictor.predict(test_values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluate\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, simply by making an http POST request. But first, we'll need to setup serializers and deserializers for passing our test_data NumPy arrays to the model behind the endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, we'll use a simple function to:\n",
    "\n",
    "1. Loop over our test dataset\n",
    "2. Split it into mini-batches of rows\n",
    "3. Convert those mini-batchs to CSV string payloads\n",
    "4. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "5. Collect predictions and convert from the CSV output our model provides into a NumPy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(val_df.to_numpy()[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = val_df.to_numpy()[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class_list = ['Benign','Bot','DoS attacks-GoldenEye','DoS attacks-Slowloris','DDoS attacks-LOIC-HTTP','Infilteration','DDOS attack-LOIC-UDP','DDOS attack-HOIC','Brute Force-Web','Brute Force-XSS','SQL Injection','DoS attacks-SlowHTTPTest','DoS attacks-Hulk','FTP-BruteForce','SSH-Bruteforce']\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "cm = confusion_matrix(actual,predictions)\n",
    "normalized_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(normalized_cm, ax=ax, annot=cm, fmt='',xticklabels=class_list,yticklabels=class_list)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confustion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's gracefully stop the deployed endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Hyperparameter Optimization (HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_hyperparameters = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": \"15\",\n",
    "    \"num_round\": \"10\",\n",
    "}\n",
    "\n",
    "output_path = 's3://{0}/{1}/output/'.format(bucket_name, prefix)\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                          hyperparameters=static_hyperparameters,\n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'eta':              ContinuousParameter(0, 1),\n",
    "    'min_child_weight': ContinuousParameter(1, 10), \n",
    "    'alpha':            ContinuousParameter(0, 2),\n",
    "    'max_depth':        IntegerParameter(1, 10),\n",
    "    'gamma':            ContinuousParameter(0, 100)\n",
    "}\n",
    "\n",
    "objective_metric_name = 'validation:merror'\n",
    "objective_type = 'Minimize'\n",
    "\n",
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=2,\n",
    "                            objective_type=objective_type,\n",
    "                            early_stopping_type='Auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n",
      "CPU times: user 3.58 s, sys: 305 ms, total: 3.88 s\n",
      "Wall time: 1h 12min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'xgboost-webtraffic-hpo-best')\n",
    "\n",
    "hpo_predictor = tuner.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.m4.xlarge',\n",
    "                         endpoint_name=tf_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_up\n",
    "sess.delete_endpoint(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "A Realistic Cyber Defense Dataset (CSE-CIC-IDS2018) https://registry.opendata.aws/cse-cic-ids2018/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
